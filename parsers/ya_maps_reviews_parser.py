import os
import time
import re
import asyncio
from typing import List, Dict, Optional, Any

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service

from bs4 import BeautifulSoup

import httpx

from dotenv import load_dotenv
from logger import logger

from database.database import SessionLocal, init_db
from database.models import Restaurant
from database.crud import (
    get_restaurant_by_notion_id,
    save_reviews_batch,
    update_restaurant_rating,
    get_reviews_stats,
    update_restaurant_link_status
)

load_dotenv('config/.env')

YA_GEO_SUGEST_API_KEY = os.getenv("YA_GEO_SUGEST_API_KEY")

BROWSER_OPTIONS = {
    "headless": "--headless=new",
    "no_sandbox": "--no-sandbox",
    "disable_dev_shm": "--disable-dev-shm-usage",
    "window_size": "--window-size=1920,1080",
    "user_agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

DEFAULT_SCROLL_ATTEMPTS = 5
DEFAULT_MAX_REVIEWS = 100
PAGE_LOAD_TIMEOUT = int(os.getenv("PAGE_LOAD_TIMEOUT", "30"))
ELEMENT_WAIT_TIMEOUT = int(os.getenv("ELEMENT_WAIT_TIMEOUT", "20"))
SCROLL_DELAY = int(os.getenv("SCROLL_DELAY", "2"))
MAX_RETRY_ATTEMPTS = int(os.getenv("MAX_RETRY_ATTEMPTS", "2"))
RETRY_DELAY = int(os.getenv("RETRY_DELAY", "5"))


def setup_driver() -> webdriver.Chrome:
    options = webdriver.ChromeOptions()

    for option in BROWSER_OPTIONS.values():
        options.add_argument(option)

    # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –±–∏–Ω–∞—Ä—å Chromium –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
    chrome_bin = os.getenv("CHROME_BIN", "/usr/bin/chromium")
    if os.path.exists(chrome_bin):
        options.binary_location = chrome_bin

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º Selenium Manager (–≤—Å—Ç—Ä–æ–µ–Ω –≤ selenium 4.x) –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ –¥—Ä–∞–π–≤–µ—Ä–∞
    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)

    return driver


def extract_review_data(review_element: BeautifulSoup) -> Optional[Dict[str, Any]]:
    try:
        author = _extract_author_name(review_element)
        user_id = _extract_user_id(review_element)
        rating = _extract_rating(review_element)
        text = _extract_review_text(review_element)
        date_iso = _extract_review_date(review_element)

        if user_id:
            yandex_review_id = user_id if len(user_id) > 7 else f"{user_id}_{date_iso[:10]}"
        else:
            yandex_review_id = f"{author}_{date_iso[:10]}"

        return {
            'author': author,
            'yandex_review_id': yandex_review_id,
            'rating': rating,
            'text': text,
            'date_iso': date_iso
        }

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–∞: {e}")
        return None


def _extract_author_name(review_element: BeautifulSoup) -> str:
    selectors = ['span[itemprop="name"]', 'div.business-review-view__author-name', 'span.business-review-view__author']
    for selector in selectors:
        elem = review_element.select_one(selector)
        if elem:
            return elem.get_text(strip=True)
    return "–ê–Ω–æ–Ω–∏–º"


def _extract_user_id(review_element: BeautifulSoup) -> Optional[str]:
    # –ò–∑ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ—Ñ–∏–ª—å
    author_container = review_element.find('div', class_='business-review-view__author-container')
    if author_container:
        for link in author_container.find_all('a', href=True):
            if '/maps/user/' in link['href']:
                return link['href'].split('/maps/user/')[-1].split('/')[0]

    # –ò–∑ –∞–≤–∞—Ç–∞—Ä–∞
    avatar_div = review_element.find('div', class_='user-icon-view__icon')
    if avatar_div and 'style' in avatar_div.attrs:
        style = avatar_div['style']
        if 'background-image' in style:
            url_match = re.search(r'url\("([^"]+)"\)', style)
            if url_match and 'get-yapic' in url_match.group(1):
                parts = url_match.group(1).split('/')
                if len(parts) >= 5:
                    return parts[4]
    return None


def _extract_rating(review_element: BeautifulSoup) -> int:
    rating_div = review_element.find('div', class_='business-rating-badge-view__stars')
    if rating_div and 'aria-label' in rating_div.attrs:
        try:
            return int(rating_div['aria-label'].split()[1])
        except (IndexError, ValueError):
            pass

    stars = review_element.find_all('span', class_='business-rating-badge-view__star _full')
    return len(stars)


def _extract_review_text(review_element: BeautifulSoup) -> str:
    selectors = ['span.spoiler-view__text-container', 'div.business-review-view__body']
    for selector in selectors:
        elem = review_element.select_one(selector)
        if elem:
            return elem.get_text(strip=True)
    return "–ù–µ—Ç —Ç–µ–∫—Å—Ç–∞"


def _extract_review_date(review_element: BeautifulSoup) -> str:
    date_elem = review_element.find('span', class_='business-review-view__date')
    if date_elem:
        meta_date = date_elem.find('meta', itemprop='datePublished')
        if meta_date and 'content' in meta_date.attrs:
            return meta_date['content']
        return date_elem.get_text(strip=True)
    return "–î–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"


def parse_yandex_reviews(url: str, max_reviews: int = -
                         1, scroll_attempts: int = DEFAULT_SCROLL_ATTEMPTS) -> List[Dict[str, Any]]:
    for attempt in range(1, MAX_RETRY_ATTEMPTS + 1):
        driver = None
        try:
            logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt}/{MAX_RETRY_ATTEMPTS}: –ø–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤")
            logger.info(f"URL: {url}")

            driver = setup_driver()
            driver.get(url)
            time.sleep(3)

            if not _verify_page_loaded(driver):
                raise Exception("–°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")

            _sort_reviews_by_newest(driver)

            if not _wait_for_reviews_loading(driver):
                raise Exception("–û—Ç–∑—ã–≤—ã –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å")

            _scroll_page_for_reviews(driver, scroll_attempts)
            reviews = _parse_reviews_from_page(driver, max_reviews)

            if reviews:
                logger.success(f"‚úì –ù–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤: {len(reviews)}")
                return reviews
            else:
                logger.warning(f"‚ö† –û—Ç–∑—ã–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt}")
                if attempt < MAX_RETRY_ATTEMPTS:
                    logger.info(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {RETRY_DELAY} —Å–µ–∫—É–Ω–¥...")
                    time.sleep(RETRY_DELAY)
                    continue
                else:
                    logger.error("–ò—Å—á–µ—Ä–ø–∞–Ω—ã –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏, –æ—Ç–∑—ã–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                    return []

        except Exception as e:
            logger.error(f"‚úó –û—à–∏–±–∫–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt}/{MAX_RETRY_ATTEMPTS}: {e}")
            if attempt < MAX_RETRY_ATTEMPTS:
                logger.info(f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {RETRY_DELAY} —Å–µ–∫—É–Ω–¥...")
                time.sleep(RETRY_DELAY)
            else:
                logger.error("–ò—Å—á–µ—Ä–ø–∞–Ω—ã –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏")
                return []

        finally:
            if driver:
                try:
                    driver.quit()
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ –¥—Ä–∞–π–≤–µ—Ä–∞: {e}")

    return []


def _sort_reviews_by_newest(driver: webdriver.Chrome) -> None:
    try:
        sort_selectors = [
            ".business-reviews-card-view__ranking .rating-ranking-view",
            ".rating-ranking-view",
            "[role='button'][aria-haspopup='true']"
        ]

        sort_btn = None
        for selector in sort_selectors:
            try:
                sort_btn = WebDriverWait(driver, 5).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, selector))
                )
                break
            except BaseException:
                continue

        if not sort_btn:
            return

        driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", sort_btn)
        time.sleep(1)
        try:
            sort_btn.click()
        except BaseException:
            driver.execute_script("arguments[0].click();", sort_btn)
        time.sleep(2)

        popup_selectors = [".rating-ranking-view__popup", ".popup", "[class*='popup']"]
        popup = None
        for selector in popup_selectors:
            try:
                popup = WebDriverWait(driver, 5).until(
                    EC.visibility_of_element_located((By.CSS_SELECTOR, selector))
                )
                break
            except BaseException:
                continue

        if popup:
            newest_selectors = [
                ".//div[text()='–ü–æ –Ω–æ–≤–∏–∑–Ω–µ']",
                ".//span[text()='–ü–æ –Ω–æ–≤–∏–∑–Ω–µ']",
                "[data-value='newest']"
            ]

            for selector in newest_selectors:
                try:
                    newest_item = popup.find_element(
                        By.XPATH if selector.startswith('.//') else By.CSS_SELECTOR, selector)
                    driver.execute_script("arguments[0].click();", newest_item)
                    time.sleep(3)
                    break
                except BaseException:
                    continue

    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–µ –æ—Ç–∑—ã–≤–æ–≤: {e}")


def _verify_page_loaded(driver: webdriver.Chrome) -> bool:
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, 'body'))
        )

        if "404" in driver.title or "–û—à–∏–±–∫–∞" in driver.title:
            logger.error("–ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –æ—à–∏–±–∫–∏")
            return False

        return True
    except Exception as e:
        logger.error(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å: {e}")
        return False


def _wait_for_reviews_loading(driver: webdriver.Chrome) -> bool:
    try:
        selectors = [
            '.business-reviews-card-view__review',
            '[class*="business-review"]',
            '[class*="review-card"]'
        ]

        for selector in selectors:
            try:
                WebDriverWait(driver, ELEMENT_WAIT_TIMEOUT).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                logger.info(f"‚úì –û—Ç–∑—ã–≤—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã (—Å–µ–ª–µ–∫—Ç–æ—Ä: {selector})")
                return True
            except BaseException:
                continue

        logger.warning("‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –æ—Ç–∑—ã–≤—ã –Ω–∏ –ø–æ –æ–¥–Ω–æ–º—É –∏–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤")
        return False

    except Exception as e:
        logger.warning(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–∂–∏–¥–∞–Ω–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –æ—Ç–∑—ã–≤–æ–≤: {e}")
        return False


def _scroll_page_for_reviews(driver: webdriver.Chrome, scroll_attempts: int) -> None:
    last_height = driver.execute_script("return document.body.scrollHeight")

    for attempt in range(scroll_attempts):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(SCROLL_DELAY)

        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height


def _parse_reviews_from_page(driver: webdriver.Chrome, max_reviews: int) -> List[Dict[str, Any]]:
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    review_elements = soup.find_all('div', class_='business-reviews-card-view__review')

    reviews = []
    for element in review_elements[:max_reviews]:
        review_data = extract_review_data(element)
        if review_data:
            reviews.append(review_data)

    return reviews


async def make_request(place: str, place_coordinates: Dict[str, Any]) -> Optional[str]:
    if not YA_GEO_SUGEST_API_KEY:
        logger.warning("YA_GEO_SUGEST_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        return None

    url = "https://suggest-maps.yandex.ru/v1/suggest"

    params = {
        "text": place,
        "print_address": 1,
        "types": "biz",
        "results": 1,
        "attrs": "uri",
        "apikey": YA_GEO_SUGEST_API_KEY
    }

    if (
        place_coordinates
        and place_coordinates.get('latitude') is not None
        and place_coordinates.get('longitude') is not None
    ):
        params["ll"] = f"{place_coordinates['latitude']},{place_coordinates['longitude']}"

    async with httpx.AsyncClient() as client:
        try:
            logger.info(f"–ü–æ–∏—Å–∫ –º–µ—Å—Ç–∞: {place}")
            response = await client.get(url, params=params)
            response.raise_for_status()
            data = response.json()

            if data.get('results'):
                logger.success(f"–ú–µ—Å—Ç–æ –Ω–∞–π–¥–µ–Ω–æ: {data['results'][0].get('title', '–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ')}")
                place_info = data['results'][0]
                yandex_uri = place_info.get('uri')
                if not yandex_uri:
                    logger.error("–í –æ—Ç–≤–µ—Ç–µ API –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç uri")
                    return None
                search_link = f"https://yandex.ru/maps/?mode=poi&poi[uri]={yandex_uri}&tab=reviews"
                return search_link
            else:
                logger.error(f"–ú–µ—Å—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {place}")
                return None

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {e}")
            return None


def parse_and_save_reviews(
    notion_id: str,
    max_reviews: int = DEFAULT_MAX_REVIEWS,
    scroll_attempts: int = DEFAULT_SCROLL_ATTEMPTS
) -> Dict[str, Any]:
    logger.info(f'–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è notion_id: {notion_id}')
    init_db()
    db = SessionLocal()

    try:
        restaurant = get_restaurant_by_notion_id(db, notion_id)
        if not restaurant:
            return {"success": False, "error": "–†–µ—Å—Ç–æ—Ä–∞–Ω —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º notion_id –Ω–µ –Ω–∞–π–¥–µ–Ω"}

        place_name = restaurant.name
        place_coordinates = {
            "latitude": restaurant.latitude,
            "longitude": restaurant.longitude
        }

        address_missing = (restaurant.address is None) or (str(restaurant.address).strip() == "")
        coords_missing = (place_coordinates["latitude"] is None) or (place_coordinates["longitude"] is None)
        if address_missing and coords_missing:
            logger.error("–£ –º–µ—Å—Ç–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∞–¥—Ä–µ—Å –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã")
            return {
                "success": False,
                "error": "–£ –º–µ—Å—Ç–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∞–¥—Ä–µ—Å –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ."
            }

        if coords_missing:
            logger.warning("–£ –º–µ—Å—Ç–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã. –ü–æ–∏—Å–∫ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ ll.")

        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤: {place_name}")

        if restaurant.yandex_maps_url and restaurant.yandex_url_status == 'ok':
            reviews_url = restaurant.yandex_maps_url
            logger.info(f"‚úì –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é URL –∏–∑ –ë–î: {reviews_url}")
        else:
            logger.info(f"üîç URL –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ë–î, –∏—â–µ–º —á–µ—Ä–µ–∑ –Ø–Ω–¥–µ–∫—Å API...")
            reviews_url = _build_reviews_url(place_name, place_coordinates)

            if not reviews_url:
                error_msg = "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ –æ—Ç–∑—ã–≤—ã (–º–µ—Å—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç–∞—Ö)"
                logger.error(error_msg)
                update_restaurant_link_status(db, restaurant.id, 'not_found')
                return {"success": False, "error": error_msg, "skip": True}

            restaurant.yandex_maps_url = reviews_url
            db.commit()
            logger.success(f"‚úì URL –Ω–∞–π–¥–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ë–î: {reviews_url}")

        reviews = parse_yandex_reviews(reviews_url, max_reviews, scroll_attempts)
        if not reviews:
            logger.warning(
                f"‚ö† –û—Ç–∑—ã–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è {place_name}. –í–æ–∑–º–æ–∂–Ω–æ, —É —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞ –ø–æ–∫–∞ –Ω–µ—Ç –æ—Ç–∑—ã–≤–æ–≤ –∏–ª–∏ –≤–æ–∑–Ω–∏–∫–ª–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º")
            update_restaurant_link_status(db, restaurant.id, 'broken')
            return {
                "success": True,  # –°—á–∏—Ç–∞–µ–º —ç—Ç–æ —É—Å–ø–µ—Ö–æ–º, –ø—Ä–æ—Å—Ç–æ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö
                "restaurant_id": restaurant.id,
                "restaurant_name": place_name,
                "reviews_found": 0,
                "reviews_new": 0,
                "total_reviews": 0,
                "avg_rating": 0,
                "warning": "–û—Ç–∑—ã–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
            }

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É")
        update_restaurant_link_status(db, restaurant.id, 'ok')
        save_result = _save_reviews_to_database(db, restaurant.id, reviews)
        _update_restaurant_statistics(db, restaurant.id, reviews)
        final_stats = _get_final_statistics(db, restaurant.id, place_name, save_result)

        logger.success(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ –¥–ª—è: {place_name}")
        return final_stats

    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}"
        logger.error(error_msg)
        try:
            if 'restaurant' in locals() and restaurant:
                update_restaurant_link_status(db, restaurant.id, 'unreachable')
        except Exception:
            pass
        return {"success": False, "error": error_msg}

    finally:
        db.close()


def _build_reviews_url(place_name: str, place_coordinates: Dict[str, Any]) -> Optional[str]:
    try:
        logger.debug(f"place_name: {place_name}")
        return asyncio.run(make_request(place_name, place_coordinates))
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ —Å—Å—ã–ª–∫–∏: {e}")
        return None


def _save_reviews_to_database(db: SessionLocal, restaurant_id: int, reviews: List[Dict[str, Any]]) -> Dict[str, Any]:
    return save_reviews_batch(db, restaurant_id, reviews)


def _update_restaurant_statistics(db: SessionLocal, restaurant_id: int, reviews: List[Dict[str, Any]]) -> None:
    if reviews:
        avg_rating = sum(r['rating'] for r in reviews) / len(reviews)
        update_restaurant_rating(db, restaurant_id, avg_rating)


def _get_final_statistics(db: SessionLocal, restaurant_id: int, place_name: str,
                          save_result: Dict[str, Any]) -> Dict[str, Any]:
    stats = get_reviews_stats(db, restaurant_id)
    return {
        "success": True,
        "restaurant_id": restaurant_id,
        "restaurant_name": place_name,
        "reviews_found": save_result["reviews_found"],
        "reviews_new": save_result["reviews_new"],
        "total_reviews": stats['total_reviews'],
        "avg_rating": stats['avg_rating']
    }


def fetch_reviews_for_all_restaurants(
    max_reviews: int = DEFAULT_MAX_REVIEWS,
    scroll_attempts: int = DEFAULT_SCROLL_ATTEMPTS,
    limit_restaurants: int = None
) -> Dict[str, Any]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤ –∏–∑ –ë–î"""
    init_db()
    db = SessionLocal()

    try:
        query = db.query(Restaurant).order_by(Restaurant.last_updated.desc())

        if limit_restaurants:
            query = query.limit(limit_restaurants)

        restaurants = query.all()
        total = len(restaurants)

        if total == 0:
            logger.warning("–í –ë–î –Ω–µ—Ç —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤")
            return {"success": False, "error": "–í –ë–î –Ω–µ—Ç —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤"}

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {total} —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤ –≤ –ë–î")
        if limit_restaurants:
            logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ {limit_restaurants} —Å–∞–º—ã—Ö —Å–≤–µ–∂–∏—Ö —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤")
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤...")

        success_count = 0
        error_count = 0
        warning_count = 0  # –†–µ—Å—Ç–æ—Ä–∞–Ω—ã –±–µ–∑ –æ—Ç–∑—ã–≤–æ–≤
        skipped_count = 0   # –†–µ—Å—Ç–æ—Ä–∞–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏
        total_new_reviews = 0
        total_found_reviews = 0

        for i, restaurant in enumerate(restaurants, 1):
            logger.info(f"\n{'=' * 60}")
            logger.info(f"[{i}/{total}] –û–±—Ä–∞–±–æ—Ç–∫–∞: {restaurant.name}")
            logger.info(f"notion_id: {restaurant.notion_id}")
            logger.info(f"{'=' * 60}")

            try:
                result = parse_and_save_reviews(
                    notion_id=restaurant.notion_id,
                    max_reviews=max_reviews,
                    scroll_attempts=scroll_attempts
                )

                if result.get("success"):
                    reviews_new = result.get('reviews_new', 0)
                    reviews_found = result.get('reviews_found', 0)

                    if result.get('warning'):
                        # –ù–µ—Ç –æ—Ç–∑—ã–≤–æ–≤, –Ω–æ –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ—à—ë–ª
                        warning_count += 1
                        logger.warning(f"‚ö† {restaurant.name}: {result.get('warning')}")
                    else:
                        # –£—Å–ø–µ—à–Ω–æ –Ω–∞–π–¥–µ–Ω—ã –æ—Ç–∑—ã–≤—ã
                        success_count += 1
                        total_new_reviews += reviews_new
                        total_found_reviews += reviews_found
                        logger.success(f"‚úì {restaurant.name}: {reviews_new} –Ω–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ {reviews_found} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö")
                else:
                    # –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞
                    if result.get('skip'):
                        skipped_count += 1
                        logger.warning(
                            f"‚äò {restaurant.name}: {result.get('error', '–†–µ—Å—Ç–æ—Ä–∞–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç–∞—Ö')}")
                    else:
                        error_count += 1
                        logger.error(f"‚úó {restaurant.name}: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")

            except Exception as e:
                error_count += 1
                logger.error(f"‚úó {restaurant.name}: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ - {str(e)}")

        logger.info("\n" + "=" * 60)
        logger.success(f"–û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        logger.info(f"‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {success_count}/{total} —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤")
        logger.info(f"‚ö† –ë–µ–∑ –æ—Ç–∑—ã–≤–æ–≤: {warning_count}/{total} —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤")
        logger.info(f"‚äò –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ –Ω–∞–π–¥–µ–Ω–æ): {skipped_count}/{total} —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤")
        logger.info(f"‚úó –û—à–∏–±–æ–∫: {error_count}/{total} —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤")
        logger.info(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤: {total_found_reviews}")
        logger.info(f"–ù–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ: {total_new_reviews}")
        logger.info("=" * 60)

        return {
            "success": True,
            "total_restaurants": total,
            "processed_successfully": success_count,
            "no_reviews": warning_count,
            "skipped": skipped_count,
            "errors": error_count,
            "total_reviews_found": total_found_reviews,
            "total_new_reviews": total_new_reviews
        }

    finally:
        db.close()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç')
    parser.add_argument('--all', action='store_true', help='–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–∑—ã–≤—ã –¥–ª—è –≤—Å–µ—Ö —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤ –∏–∑ –ë–î')
    parser.add_argument('--notion-id', type=str, help='Notion ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞')
    parser.add_argument('--max-reviews', type=int, default=DEFAULT_MAX_REVIEWS, help='–ú–∞–∫—Å–∏–º—É–º –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Ä–µ—Å—Ç–æ—Ä–∞–Ω')
    parser.add_argument(
        '--scroll-attempts',
        type=int,
        default=DEFAULT_SCROLL_ATTEMPTS,
        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–æ–∫—Ä—É—Ç–∫–∏')
    parser.add_argument('--limit', type=int, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ)')

    args = parser.parse_args()

    if args.all:
        result = fetch_reviews_for_all_restaurants(
            max_reviews=args.max_reviews,
            scroll_attempts=args.scroll_attempts,
            limit_restaurants=args.limit
        )
        if result.get("success"):
            logger.success("–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        else:
            logger.error(f"–û—à–∏–±–∫–∞: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")

    elif args.notion_id:
        result = parse_and_save_reviews(
            notion_id=args.notion_id,
            max_reviews=args.max_reviews,
            scroll_attempts=args.scroll_attempts
        )
        if result["success"]:
            logger.success("–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            logger.info(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {result['reviews_new']} –Ω–æ–≤—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ {result['reviews_found']} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö")
            logger.info(f"–°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥: {result['avg_rating']:.1f}")
        else:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {result['error']}")

    else:
        logger.info("\n–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:")
        logger.info("  python ya_maps_reviews_parser.py --all")
        logger.info("  python ya_maps_reviews_parser.py --all --limit 50  # —Ç–æ–ª—å–∫–æ 50 —Å–∞–º—ã—Ö —Å–≤–µ–∂–∏—Ö")
        logger.info("  python ya_maps_reviews_parser.py --notion-id 18e4fad2-f8ee-805d-8a7c-c34099c6d48f")
        logger.info("  python ya_maps_reviews_parser.py --all --max-reviews 50 --scroll-attempts 3")
